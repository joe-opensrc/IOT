#!/bin/bash 

### update function uses 'Page x of n' and arithmetic to keep update requests low.; 
### date from json appears to be incorrect in at least one case (TristanAndIseult)
### all functions and loops assume ordered data; i.e., the web pages provide the order (newest to oldest),
### and the db reflects that.  If order changes then all bets are off. (hashes are complicated; I can't be bothered here ;))
### the script is not optimised, and is dependency heavy for a shell script... but it's functional :D
### also please forgive the BBC for it's terrible web design, and the pernicious knock-on effect it has on shell scripts ;P
### that is, the mp3 links, and the full descriptions of each episode are not maintained with the other data inside the json object
### in order to limit the number of requests to the BBC site, we try and limt updates up-to the page which contains the last known episode.

# base of operation
IOT_ROOT=~/bin/IOT  
# example location to store episodes
IOT_HOME=~/Audio/InOurTime
# a place where we put a simple backup of existing iot.db
IOT_BCK="${IOT_ROOT}/bck"
# podcast home page
IOT_URI="https://www.bbc.co.uk/programmes/b006qykl/episodes/player"
# our InOurTime Database
IOT_DB="${IOT_ROOT}/iot.db"

IOT_TDIR="/var/tmp/iot/fetch"

# echo command instead of running it
dryrun=""

# get latest episode id from local iot.db
epi_id_latest_db=$(head -n 1 "${IOT_DB}" | cut -d'|' -f 1 )

tpf="${IOT_TDIR}/bbc-iot-1.html"

# fs side-effect nightmare function :)
function pageFetchParse(){

  local tpf="${IOT_TDIR}/bbc-iot-${1}.html"
  local tpfp="${tpf}.psv"

  if [[ ${2} -ne 0 && -e "${tpf}" ]]
  then
    echo "Existing file: ${tpf}" >&2
  else
    echo "Fetching page-${n} from BBC" >&2
    mkdir -p "${IOT_TDIR}" 2>/dev/null
    wget -q -O${tpf} "${IOT_URI}?page=${n}" 
  fi

  cat "${tpf}" | grep 'RadioSeries' | jq -jc '.episode[] as $epi | $epi.identifier, "|", $epi.name, "|", $epi.url, "|", $epi.publication.startDate, "|", $epi.description, "\n"' >"${tpfp}"  

}

function checkIdInPage(){

  local epi_id="${1}"
  local page_id="${2}" 

  local tpf="${IOT_TDIR}/bbc-iot-${page_id}.html"
  local tpfp="${tpf}.psv"

  # echo ${tpfp}
  grep "^${epi_id}" "${tpfp}" &>/dev/null
  return $?

}

function updateDB(){

  n=1
  # Must ask BBC first
  local tpf="${IOT_TDIR}/bbc-iot-1.html"
  pageFetchParse 1 0 # creates files :S # ${2}==0 forces
  plast=$( grep 'pagination__page--last' -A 1 "${tpf}" | grep 'Page [0-9]\+' -o | cut -d' ' -f 2 )

  checkIdInPage ${epi_id_latest_db} ${n} 
  # compare the latest known episode id from the db, with the site
  until [[ $? -eq 0 || ${n} -ge ${plast} ]]
  do
    (( n++ )) 
    sleep 1
    pageFetchParse ${n}
    checkIdInPage ${epi_id_latest_db} ${n} 
    
  done 

  new_db_ents=()
  x=1
  while [[ ${x} -le ${n} ]]
  do

    IFS=$'\n'
    for p in $( cat "${IOT_TDIR}/bbc-iot-${x}.html.psv" )
    do  
      id=$(echo "${p}" | cut -d'|' -f 1 )
      if [[ ${id} != ${epi_id_latest_db} ]]
      then
        url=$( echo "${p}" | cut -d'|' -f 3 )
        # echo "URL: ${url}" >&2
        epifile="${IOT_TDIR}/bbc-iot-epi-${id}.html"
        if [[ -e "${epifile}" ]]
        then
          echo "Existing epi file: ${epifile}" >&2
        else
          echo "Fetching epi site: ${url}" >&2
          wget -q -O"${epifile}" "${url}"
        fi
      
        # mp3 urls are not kept in the json object; so we parse the page
        id_mp3_url="$( grep 'open\.live' -A 1 ${epifile} | grep -v 'download-low' | sed -nre '/open\.live/ N; s/\n/ /; s/.*href="\/\/(.*?\.mp3).*?- ([a-z0-9]+)\.mp3.*/\2|https\:\/\/\1/ p' )"

        desc_long="$( grep 'synopsis-toggle__long' -A 1 ${epifile} | sed -rne 's|.*?<p>(.*?)</p>|\1| p' )"
        additional_fields="$( join -t'|' <( echo -ne "${id_mp3_url:-${id}|NOTCURRENTLYAVAILABLE}" ) <( echo -ne "${id}|${desc_long}" ) )"

        new_db_ents+=( "$( join -t'|' <( echo ${p} ) <( echo -ne ${additional_fields} ) )" )
        
      else
        break
      fi 

    done
    IFS=$' \t\n'

    (( x++ ))

  done

  if [[ ${do_backup} -eq 0 ]]
  then
    #noddy backup of last db
    bname="${IOT_BCK}/iot_db_bck_$(date '+%Y%m%d_%H%M%S').gz"

    echo -ne "Backup of iot.db to ${bname}\n" >&2
    gzip -c "${IOT_DB}" >"${bname}"
  fi

  # Membound...but we're looking at single-digit megabytes...besides BBC have broken it by removing downloads;
  # they appear to have moved it about internally after the podcast got "promoted".
  if [[ ${#new_db_ents[*]} -gt 0 ]]
  then
    IFS=$'\n'
      echo -ne "Merging:\n${new_db_ents[*]}" >&2
      diff --line-format='%L' <( echo "${new_db_ents[*]}" ) "${IOT_DB}" | sponge "${IOT_DB}"
    IFS=$' \t\n'
  fi

  echo -ne "Update...Finished\n" >&2

}

## -B := do backup the db; #YOLO.
## -u := update the database
## -d := supply the episode description to the fzf filter
## -n := dry run
nth="2"
do_backup=1
OPTIND=
while getopts 'Bdnu' flag
do
  case "${flag}" in
     u) updateDB; exit 0;; # ${OPTARG}; exit 0;; 
     d) nth="2,7";;
     n) dryrun="echo ";;
     B) do_backup=0;;
    \?) echo "ARGS!" >&2; exit 1;;
    \:) echo "EGG" >&2; exit 1;;
  esac
  shift $(( ${OPTIND} - 1 ))
done

choice=$( cat ${IOT_ROOT}/iot.db | fzf --no-hscroll -0 -1 -d'\|' -e +m +s --cycle --reverse -i --with-nth=${nth} --preview='echo -e \"{2}\\n{7}\\n{6}\" | fmt -w $(tput cols)' --preview-window='wrap' --bind='alt-j:preview-page-down,alt-k:preview-page-up' )

name="$(         echo ${choice} | cut -d'|' -f 2 )"
url="$(          echo ${choice} | cut -d'|' -f 6 )"
pub_date="$(     echo ${choice} | cut -d'|' -f 4  | cut -c 1-10 | tr -d '-' )"
epi_url="$(      echo ${choice} | cut -d'|' -f 3 )" 

if [[ -n "${name}" ]]
then
  echo "Trying: ${name}" >&2
  name=$( echo "${name}" | tr -d "'" )
  exists="$( for x in ${name}; do echo -n "${x^}"; done )"

  f="$( ls -1 ${IOT_HOME}/InOurTime-*${exists}* 2>/dev/null  )"

  if [[ $? -eq 0 ]]
  then
      echo "File Appears To Already Exist: $( basename ${f} )"
  else

    (
      cd ${IOT_HOME}
      if [[ "${url}" == "NOTCURRENTLYAVAILABLE" ]]
      then
        # an eventual version will update the db maybe?
        echo -ne "MP3 Not Currently Available.  Saving Placeholder File\n" >&2
        ${dryrun} echo -ne "${epi_url}" >"${IOT_HOME}/InOurTime-${pub_date}-${exists}.phold"
      else
        ${dryrun} curl -L -o "InOurTime-${pub_date}-${exists}.mp3" --progress-bar "${url}" # -L := allow redirects, -J := Content-Disposition filename 
      fi
    ) 
 
  fi

else
  echo -ne "\nNo Episode Selected.  Nothing to do.\n\n"
fi

