#!/bin/bash 

### update function uses 'Page x of n' and arithmetic to keep update requests low.; 
### date from json appears to be incorrect in at least one case (TristanAndIseult)
### all functions and loops assume ordered data; i.e., the web pages provide the order (newest to oldest),
### and the db reflects that.  If order changes then all bets are off. (hashes are complicated; I can't be bothered here ;))
### the script is not optimised, and is dependency heavy for a shell script... but it's functional :D
### also please forgive the BBC for it's terrible web design, and the pernicious knock-on effect it has on shell scripts ;P
### that is, the mp3 links, and the full descriptions of each episode are not maintained with the other data inside the json object
### in order to limit the number of requests to the BBC site, we try and limt updates up-to the page which contains the last known episode.

# base of operation
IOT_ROOT=~/bin/IOT  
# example location to store episodes
IOT_HOME=~/Audio/InOurTime
# a place where we put a simple backup of existing iot.db
IOT_BCK="${IOT_ROOT}/bck"
# podcast home page
IOT_URI="https://www.bbc.co.uk/programmes/b006qykl/episodes/downloads"
# our InOurTime Database
IOT_DB="${IOT_ROOT}/iot.db"

# echo command instead of running it
dryrun=""

# used to update iot.db
# tries to /not/ hammer the website
# IOTFetch -u will update until the latest known episode
# user can do a full update with: IOTFetch -u 0
# TODO: logic is a bit convoluted; could be 'normalized'
function updateDB(){
 
  ans="n"
  cont=1
  full=${1:-1}

  umsg="You are requesting a partial update of the BBC In Our Time Podcast, are you sure?"
  if [[ ${full} -eq 0 ]]
  then
    umsg="You are requesting a full update of the BBC In Our Time Podcase, are you sure?"
  fi

  echo -ne "\nYou are about to parse the BBC In Our Time Podcast Archive\n\n"
  read -p "Are You Sure [y/N]? " ans

  case "${ans}" in
    Y|y) cont=0;;
      *) echo -ne "...Aborted.\n\n";
         exit 0;;
  esac

  if [[ ${cont} -eq 0 ]]
  then 
    tdb=$( mktemp )

    # get latest episode id from local iot.db
    epi_id_latest_db=$(head -n 1 "${IOT_DB}" | cut -d'|' -f 1 )

    # this is the upper bound on pages we need to scan to get the full archive
    page_last=
    page_last=$( wget -q -O- "${IOT_URI}?page=1" | grep 'pagination__page--last' -A 1 | grep 'Page [0-9]\+' -o | cut -d' ' -f 2 )

    if [[ -n "${page_last}" ]]
    then

      # for each page
      for n in $( seq 1 ${page_last} )
      do

        # mem bound potential issues obvs.
        # we always parse one page; the first
        page="$( wget -O- "${IOT_URI}?page=${n}" )"
        if [[ -n "${page}" ]]
        then

          #get the details from the RadioSeries json found in the page using 'jq'
          rs_lines="$( echo "${page}" | grep 'RadioSeries' | jq -jc '.hasPart[] as $epi | $epi.identifier, "|", $epi.name, "|", $epi.url, "|", $epi.publication.startDate, "|", $epi.description, "\n"' )" 

          # compare the latest known episode id from the db, with the site
          epi_id_latest_web=$( echo "${rs_lines}" | head -n 1 | cut -d'|' -f 1 )
          if [[ "${epi_id_latest_db}" == "${epi_id_latest_web}" ]]
          then
            echo -ne "\nALREADY UP-TO-DATE!  Nothing to Do...Aborting\n\n"
            exit 0
          fi

          # mp3 urls are not kept in the json object; so we parse the page
          mp3_urls="$( echo "${page}" | grep 'open\.live' -A 1 | grep -v 'download-low' | sed -nre '/open\.live/ N; s/\n/ /; s/.*href="\/\/(.*?\.mp3).*?- ([a-z0-9]+)\.mp3.*/\2|\1/ p' )"


          # new db line is the merging of the json and parsed output according to our pipe delimted format.
          db_lines=$( join -t'|' <( echo -n "${rs_lines}") <( echo -n "${mp3_urls}" ) )

          # if not a full search; process the first page db lines;
          # otherwise fetch more episodes
          if [[ ${full} -ne 0 ]]        
          then

            # if the current page contains the last known id; don't fetch anymore pages
            echo "${db_lines}" | grep "${epi_id_latest_db}" &>/dev/null
            if [[ $? -eq 0 ]]
            then

              IFS=$'\n'
              #for each epi we're updating: download epi page and extract long description join to existing db entry
              for epi in ${db_lines}
              do 

                # episode id 
                eid=$( echo "${epi}" | cut -d'|' -f 1 )

                # extract / filter only those we haven't yet seen
                if [[ "${eid}" != "${epi_id_latest_db}" ]]
                then 

                  # fetch long desc and put into tmp db of updates
                  title=$( echo "${epi}" | cut -d'|' -f 2 )
                    url=$( echo "${epi}" | cut -d'|' -f 3 )
                  
                  desc_long="$( wget -q -O- ${url} | grep 'synopsis-toggle__long' -A 1 | sed -rne 's|.*?<p>(.*?)</p>|\1| p' )"

                  new_db_ent="$( join -t'|' <( echo -n "${epi}" ) <( echo -n "$eid|${desc_long}" ) )"

                  echo "${eid}|${title}"
                  echo "${new_db_ent}" >>"${tdb}"

                else
                  # ordered data allows us to stop processing episodes here
                  # as the latest episode id has already been seen 
                  break
                fi
              done
              IFS=$' \t\n'
              
              # stop processing pages   
              break

            fi
          fi

          # cycle round to fetch more episodes
          # pause so as not to hammer the website ;)
          sleep 1
  
        else 
          echo -ne "\nSomething Bad With Get Page :(\n\n"
          exit 1
        fi

      done
      
    fi
 
  fi

  # tdb either contains all entries, having scanned full site
  # or full pages upto that which contains the most recent entry from the db
  # dont merge yet; act on tdb

  #noddy backup of last db
  gzip -c "${IOT_DB}" >"${IOT_BCK}/iotdb_$(date '+%Y%m%d_%H%M%S').bck"

  #merge new list with existing
  tdb2="$( mktemp )"
  diff --line-format='%L' "${tdb}" "${IOT_DB}" >"${tdb2}"

  #replace existing
  mv "${tdb2}" "${IOT_DB}"
}


  ## args -- simple parsing 
  ## -u := update the database
  ## -d := supply the episode description to the fzf filter
  nth="2"
  case "${1}" in
    -u) updateDB ${2}; exit 0;;
    -d) nth="2,7";;
    -n) dryrun="echo ";;
  esac

choice=$( cat ${IOT_ROOT}/iot.db | fzf --no-hscroll -0 -1 -d'\|' -e +m +s --cycle --reverse -i --with-nth=${nth} --preview='echo -e \"{2}\\n{7}\" | fmt -w $(tput cols)' --preview-window='wrap' --bind='alt-j:preview-page-down,alt-k:preview-page-up' )

name="$(         echo ${choice} | cut -d'|' -f 2 )"
url="https://$(  echo ${choice} | cut -d'|' -f 6 )"

if [[ -n "${name}" ]]
then
  echo "Trying: ${name}"
  name=$( echo "${name}" | tr -d "'" )
  exists="$( for x in ${name}; do echo -n "${x^}"; done )"

  f="$( ls -1 ${IOT_HOME}/*${exists}* 2>/dev/null  )"

  if [[ $? -eq 0 ]]
  then
      echo "File Appears To Already Exist: $( basename ${f} )"
  else

    (
      cd ${IOT_HOME}
      ${dryrun} curl -J -O -L --progress-bar "${url}" # -L := allow redirects, -J := Content-Disposition filename 
    ) 
 
  fi

else
  echo -ne "\nNo Episode Selected.  Nothing to do.\n\n"
fi

